{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPui3CEaW6P0Uh/1J+z7r8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankumar211/Native-Language-Identification-Project/blob/main/Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: Install Condacolab\n",
        "print(\"Installing Condacolab...\")\n",
        "!pip install condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "Xqt49zNWouZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Install Conda Packages\n",
        "import condacolab\n",
        "condacolab.check()\n",
        "print(\"Conda is active.\")\n",
        "\n",
        "print(\"Installing MFA using Conda (this may take a few minutes)...\")\n",
        "!conda install -c conda-forge montreal-forced-aligner -y\n",
        "print(\"MFA installed successfully.\")"
      ],
      "metadata": {
        "id": "k3gDeiRYo6o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Install Pip Packages\n",
        "import sys\n",
        "python_executable_path = sys.executable\n",
        "print(f\"Using Python executable at: {python_executable_path}\")\n",
        "\n",
        "print(\"Installing all Python libraries (transformers, librosa, etc.) using conda's pip...\")\n",
        "!{python_executable_path} -m pip install transformers torch torchaudio librosa soundfile huggingface_hub requests beautifulsoup4 tqdm scikit-learn\n",
        "print(\"All Python libraries installed successfully.\")"
      ],
      "metadata": {
        "id": "1CifuYOxpKF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Verify Install & Set Backend\n",
        "import datasets\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# 1. Verify MFA\n",
        "print(\"\\nVerifying MFA installation (this should show the help menu)...\")\n",
        "!mfa model download --help\n",
        "\n",
        "# 2. Set Audio Backend (Fixes the torchcodec bug)\n",
        "datasets.config.AUDIO_DECODER = \"torchaudio\"\n",
        "print(f\"\\nAudio decoder set to: {datasets.config.AUDIO_DECODER}\")"
      ],
      "metadata": {
        "id": "3Gy4De9DpS91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Download & Unzip Audio Data\n",
        "from huggingface_hub import hf_hub_download\n",
        "import zipfile\n",
        "import os\n",
        "print(\"Downloading dataset 'DarshanaS/IndicAccentDb' (3.2 GB)...\")\n",
        "zip_path = hf_hub_download(repo_id=\"DarshanaS/IndicAccentDb\", filename=\"IndicAccentDB.zip\", repo_type=\"dataset\")\n",
        "extract_dir = \"data\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "print(\"Extracting to 'data' folder...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "-aZP0nE3pXXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Parse Transcripts\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Parsing harvard_sentences.html...\")\n",
        "url = \"https://www.cs.columbia.edu/~hgs/audio/harvard.html\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "text = soup.get_text()\n",
        "lines = text.split('\\n')\n",
        "all_harvard_sentences = []\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    if not line or line.startswith(\"List \"): continue\n",
        "    sentence = line.upper().replace('.', '').replace(',', '').replace('?', '')\n",
        "    all_harvard_sentences.append(sentence)\n",
        "print(f\"Successfully parsed {len(all_harvard_sentences)} sentences.\")\n",
        "\n",
        "print(\"\\nCreating transcript map...\")\n",
        "all_file_paths = glob.glob(\"data/**/*.wav\", recursive=True)\n",
        "transcript_map = {}\n",
        "list_pattern = re.compile(r'_List(\\d+)_(\\d+)\\.wav', re.IGNORECASE)\n",
        "num_pattern = re.compile(r'\\((\\d+)\\)\\.wav', re.IGNORECASE)\n",
        "for audio_path in all_file_paths:\n",
        "    filename = os.path.basename(audio_path)\n",
        "    sentence_index = -1\n",
        "    list_match = list_pattern.search(filename)\n",
        "    if list_match:\n",
        "        list_num = int(list_match.group(1)); sent_num = int(list_match.group(2))\n",
        "        sentence_index = (list_num - 1) * 10 + (sent_num - 1)\n",
        "    num_match = num_pattern.search(filename)\n",
        "    if num_match:\n",
        "        sent_num = int(num_match.group(1)); sentence_index = sent_num - 1\n",
        "    if 0 <= sentence_index < len(all_harvard_sentences):\n",
        "        transcript_map[audio_path] = all_harvard_sentences[sentence_index]\n",
        "clean_file_list = list(transcript_map.keys())\n",
        "print(f\"Found {len(clean_file_list)} clean files for alignment.\")"
      ],
      "metadata": {
        "id": "QfWyhuRwp0F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Create 'data_clean' Corpus\n",
        "import shutil\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"Creating a 'data_clean' corpus for alignment...\")\n",
        "clean_dir = \"data_clean\"\n",
        "os.makedirs(clean_dir, exist_ok=True)\n",
        "files_written = 0\n",
        "for audio_path in tqdm(clean_file_list, desc=\"Copying clean files\"):\n",
        "    try:\n",
        "        sentence_text = transcript_map[audio_path]\n",
        "        base_filename = os.path.basename(audio_path)\n",
        "        lab_filename = base_filename.replace('.wav', '.lab')\n",
        "        new_wav_path = os.path.join(clean_dir, base_filename)\n",
        "        new_lab_path = os.path.join(clean_dir, lab_filename)\n",
        "        shutil.copy(audio_path, new_wav_path)\n",
        "        with open(new_lab_path, 'w') as f:\n",
        "            f.write(sentence_text)\n",
        "        files_written += 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "print(f\"\\nClean corpus created. Copied {files_written} .wav/.lab pairs.\")"
      ],
      "metadata": {
        "id": "Fe6di9Uvp4Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: Download MFA Models\n",
        "print(\"Downloading MFA models (Dictionary and Acoustic)...\")\n",
        "!mfa model download dictionary english_us_arpa\n",
        "!mfa model download acoustic english_mfa\n",
        "print(\"MFA models downloaded successfully.\")"
      ],
      "metadata": {
        "id": "CP6LwzXYqAsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: Run Forced Alignment\n",
        "print(\"Starting the Montreal Forced Aligner (MFA)...\")\n",
        "print(\"This will process all 3,207 clean files.\")\n",
        "!mfa align data_clean english_us_arpa english_mfa data_aligned\n",
        "print(\"\\n--- MFA Alignment Complete! ---\")"
      ],
      "metadata": {
        "id": "Azd_GSJOqJMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Load Task 1 Results from Google Drive\n",
        "import numpy as np\n",
        "import json\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Mounting Google Drive to load Task 1 results...\")\n",
        "drive.mount('/content/drive')\n",
        "save_path = '/content/drive/MyDrive/Colab_Project_Data/'\n",
        "print(\"Loading processed Task 1 data from Google Drive...\")\n",
        "\n",
        "try:\n",
        "    X_hubert_sentence = np.load(os.path.join(save_path, 'X_hubert.npy'))\n",
        "    X_mfcc_sentence = np.load(os.path.join(save_path, 'X_mfcc.npy'))\n",
        "    y_labels_sentence = np.load(os.path.join(save_path, 'y_labels.npy'))\n",
        "    with open(os.path.join(save_path, 'label_to_int.json'), 'r') as f:\n",
        "        label_to_int = json.load(f)\n",
        "    int_to_label = {int(i): name for name, i in label_to_int.items()}\n",
        "    print(\"--- Task 1 Data Loaded Successfully ---\")\n",
        "\n",
        "    print(\"\\nRe-calculating Task 1 (Sentence) accuracies...\")\n",
        "    X_mfcc_train_s, X_mfcc_test_s, y_train_s, y_test_s = train_test_split(\n",
        "        X_mfcc_sentence, y_labels_sentence, test_size=0.2, random_state=42, stratify=y_labels_sentence\n",
        "    )\n",
        "    X_hubert_train_s, X_hubert_test_s, _, _ = train_test_split(\n",
        "        X_hubert_sentence, y_labels_sentence, test_size=0.2, random_state=42, stratify=y_labels_sentence\n",
        "    )\n",
        "    mfcc_scaler_s = StandardScaler().fit(X_mfcc_train_s)\n",
        "    mfcc_svm_s = SVC(random_state=42).fit(mfcc_scaler_s.transform(X_mfcc_train_s), y_train_s)\n",
        "    acc_mfcc = accuracy_score(y_test_s, mfcc_svm_s.predict(mfcc_scaler_s.transform(X_mfcc_test_s)))\n",
        "    hubert_scaler_s = StandardScaler().fit(X_hubert_train_s)\n",
        "    hubert_svm_s = SVC(random_state=42).fit(hubert_scaler_s.transform(X_hubert_train_s), y_train_s)\n",
        "    acc_hubert = accuracy_score(y_test_s, hubert_svm_s.predict(hubert_scaler_s.transform(X_hubert_test_s)))\n",
        "    print(f\"Task 1 Accuracies restored: MFCC={acc_mfcc*100:.2f}%, HuBERT={acc_hubert*100:.2f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"--- ERROR loading from Drive: {e} ---\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "F5Jmclwpqcm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Load HuBERT Model\n",
        "import torch\n",
        "from transformers import AutoFeatureExtractor, HubertModel\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "target_sr = 16000\n",
        "ssl_ckpt = 'facebook/hubert-base-ls960'\n",
        "print(f\"Loading model: {ssl_ckpt} onto device...\")\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(ssl_ckpt)\n",
        "hubert_model = HubertModel.from_pretrained(ssl_ckpt).to(device)\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "tRSPRdrpreKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Manual TextGrid Parser\n",
        "import re\n",
        "\n",
        "def parse_textgrid(file_path):\n",
        "    \"\"\"\n",
        "    A simple parser for .TextGrid files to extract word intervals.\n",
        "    Returns a list of (start_time, end_time, word) tuples.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    intervals = []\n",
        "    # Find all interval blocks\n",
        "    # We look for \"xmin = ...\", \"xmax = ...\", \"text = ...\" patterns\n",
        "    for match in re.finditer(\n",
        "        r\"intervals\\s*\\[\\d+\\]:\\s*\"\n",
        "        r\"xmin\\s*=\\s*([\\d\\.]+)\\s*\"\n",
        "        r\"xmax\\s*=\\s*([\\d\\.]+)\\s*\"\n",
        "        r\"text\\s*=\\s*\\\"(.*?)\\\"\\s*\",\n",
        "        content,\n",
        "        re.DOTALL\n",
        "    ):\n",
        "        start_time = float(match.group(1))\n",
        "        end_time = float(match.group(2))\n",
        "        word = match.group(3).strip()\n",
        "\n",
        "        # Skip silence (which MFA often marks as \"\")\n",
        "        if word:\n",
        "            intervals.append((start_time, end_time, word))\n",
        "\n",
        "    return intervals\n",
        "\n",
        "print(\"TextGrid parser function defined.\")\n",
        "# Test it on one file\n",
        "example_grid = glob.glob(\"data_aligned/*.TextGrid\")[0]\n",
        "print(f\"Parsing example file: {example_grid}\")\n",
        "example_words = parse_textgrid(example_grid)\n",
        "print(f\"Found words: {example_words[:5]}...\")"
      ],
      "metadata": {
        "id": "hVnWhGABrpGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Run Word-Level Feature Extraction\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "print(\"Starting WORD-LEVEL feature extraction...\")\n",
        "textgrid_files = glob.glob(\"data_aligned/*.TextGrid\")\n",
        "print(f\"Found {len(textgrid_files)} .TextGrid files to process.\")\n",
        "\n",
        "hubert_word_features = []\n",
        "mfcc_word_features = []\n",
        "word_labels = []\n",
        "\n",
        "for grid_path in tqdm(textgrid_files, desc=\"Processing words\"):\n",
        "    try:\n",
        "        base_name = os.path.basename(grid_path).replace('.TextGrid', '.wav')\n",
        "        wav_path = os.path.join(\"data_clean\", base_name)\n",
        "        original_path = glob.glob(f\"data/**/{base_name}\", recursive=True)[0]\n",
        "        label_string = os.path.basename(os.path.dirname(original_path))\n",
        "        label_int = label_to_int[label_string]\n",
        "\n",
        "        waveform_raw, sr_raw = librosa.load(wav_path, sr=None)\n",
        "\n",
        "        # --- Use our new parser ---\n",
        "        word_segments = parse_textgrid(grid_path)\n",
        "        if not word_segments: continue\n",
        "\n",
        "        for (start_time, end_time, word) in word_segments:\n",
        "            if (end_time - start_time) < 0.1: continue # Skip very short words\n",
        "\n",
        "            start_sample = int(start_time * sr_raw); end_sample = int(end_time * sr_raw)\n",
        "            word_audio_raw = waveform_raw[start_sample:end_sample]\n",
        "\n",
        "            # MFCC\n",
        "            mfccs = librosa.feature.mfcc(y=word_audio_raw, sr=sr_raw, n_mfcc=20)\n",
        "            mfcc_word_features.append(np.mean(mfccs, axis=1))\n",
        "\n",
        "            # HuBERT\n",
        "            word_audio_16k = librosa.resample(word_audio_raw, orig_sr=sr_raw, target_sr=target_sr)\n",
        "            inputs = feature_extractor(word_audio_16k, sampling_rate=target_sr, return_tensors='pt').to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = hubert_model(inputs.input_values)\n",
        "            hubert_word_features.append(outputs.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy())\n",
        "\n",
        "            word_labels.append(label_int)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {grid_path}: {e}\")\n",
        "\n",
        "X_hubert_words = np.array(hubert_word_features)\n",
        "X_mfcc_words = np.array(mfcc_word_features)\n",
        "y_labels_words = np.array(word_labels)\n",
        "\n",
        "print(f\"\\nWord-level feature extraction complete!\")\n",
        "print(f\"HuBERT word feature matrix shape: {X_hubert_words.shape}\")\n",
        "print(f\"MFCC word feature matrix shape: {X_mfcc_words.shape}\")\n",
        "print(f\"Word labels array shape: {y_labels_words.shape}\")"
      ],
      "metadata": {
        "id": "zoDr5AdDr1lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 14: Run Word-Level Train/Test/Compare\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(f\"--- Task 3: Word-Level vs. Sentence-Level ---\")\n",
        "\n",
        "X_mfcc_train_w, X_mfcc_test_w, y_train_w, y_test_w = train_test_split(\n",
        "    X_mfcc_words, y_labels_words, test_size=0.2, random_state=42, stratify=y_labels_words\n",
        ")\n",
        "X_hubert_train_w, X_hubert_test_w, _, _ = train_test_split(\n",
        "    X_hubert_words, y_labels_words, test_size=0.2, random_state=42, stratify=y_labels_words\n",
        ")\n",
        "print(f\"\\nTotal sentence samples: {len(y_labels_sentence)}\")\n",
        "print(f\"Total word samples: {len(y_labels_words)}\")\n",
        "\n",
        "mfcc_scaler_w = StandardScaler().fit(X_mfcc_train_w)\n",
        "hubert_scaler_w = StandardScaler().fit(X_hubert_train_w)\n",
        "\n",
        "print(\"Training WORD-LEVEL MFCC Model (SVM)...\")\n",
        "mfcc_svm_w = SVC(random_state=42).fit(mfcc_scaler_w.transform(X_mfcc_train_w), y_train_w)\n",
        "y_pred_mfcc_w = mfcc_svm_w.predict(mfcc_scaler_w.transform(X_mfcc_test_w))\n",
        "acc_mfcc_w = accuracy_score(y_test_w, y_pred_mfcc_w)\n",
        "\n",
        "print(\"Training WORD-LEVEL HuBERT Model (SVM)...\")\n",
        "hubert_svm_w = SVC(random_state=42).fit(hubert_scaler_w.transform(X_hubert_train_w), y_train_w)\n",
        "y_pred_hubert_w = hubert_svm_w.predict(hubert_scaler_w.transform(X_hubert_test_w))\n",
        "acc_hubert_w = accuracy_score(y_test_w, y_pred_hubert_w)\n",
        "\n",
        "print(\"\\n\\n--- FINAL COMPARISON (Task 3) ---\")\n",
        "print(\"                          | MFCCs      | HuBERT\")\n",
        "print(f\"Sentence-Level Accuracy:    | {acc_mfcc * 100:.2f}%     | {acc_hubert * 100:.2f}%\")\n",
        "print(f\"Word-Level Accuracy:        | {acc_mfcc_w * 100:.2f}%     | {acc_hubert_w * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "giW_s0p88mXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}