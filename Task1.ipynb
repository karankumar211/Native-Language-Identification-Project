{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP494L4UCFCXz2WSCHnjBee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankumar211/Native-Language-Identification-Project/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6Fd_gGHiR17"
      },
      "outputs": [],
      "source": [
        "# CELL 1: Install Pip Packages\n",
        "print(\"Installing all Python libraries for Task 1...\")\n",
        "# We install torchaudio (the stable audio loader)\n",
        "!pip install datasets transformers torch torchaudio librosa soundfile huggingface_hub requests beautifulsoup4 tqdm scikit-learn\n",
        "print(\"All Python libraries installed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Download, Unzip, and Prepare Data\n",
        "from huggingface_hub import hf_hub_download\n",
        "import zipfile\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"Downloading dataset 'DarshanaS/IndicAccentDb' (3.2 GB)...\")\n",
        "zip_path = hf_hub_download(\n",
        "    repo_id=\"DarshanaS/IndicAccentDb\",\n",
        "    filename=\"IndicAccentDB.zip\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "print(f\"Dataset downloaded to: {zip_path}\")\n",
        "\n",
        "extract_dir = \"data\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "print(f\"Extracting to '{extract_dir}' folder...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# --- Setup file lists and labels ---\n",
        "all_file_paths = glob.glob(\"data/**/*.wav\", recursive=True)\n",
        "print(f\"Found {len(all_file_paths)} audio files.\")\n",
        "\n",
        "label_names = sorted(list(set([os.path.basename(os.path.dirname(p)) for p in all_file_paths])))\n",
        "label_to_int = {name: i for i, name in enumerate(label_names)}\n",
        "int_to_label = {i: name for i, name in enumerate(label_names)}\n",
        "print(f\"Label mapping: {label_to_int}\")"
      ],
      "metadata": {
        "id": "xhouPxGEiZA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Load HuBERT Models\n",
        "import torch\n",
        "from transformers import AutoFeatureExtractor, HubertModel\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "target_sr = 16000\n",
        "\n",
        "# --- 1. Load HuBERT (Base) for feature extraction ---\n",
        "ssl_ckpt = 'facebook/hubert-base-ls960'\n",
        "print(f\"Loading model: {ssl_ckpt} onto device...\")\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(ssl_ckpt)\n",
        "hubert_model = HubertModel.from_pretrained(ssl_ckpt).to(device)\n",
        "print(\"Base HuBERT model loaded successfully.\")\n",
        "\n",
        "# --- 2. Load HuBERT (Layer-wise) ---\n",
        "print(\"Loading HuBERT model for layer-wise extraction...\")\n",
        "hubert_model_layers = HubertModel.from_pretrained(\n",
        "    ssl_ckpt,\n",
        "    output_hidden_states=True\n",
        ").to(device)\n",
        "print(\"Layer-wise HuBERT model loaded.\")"
      ],
      "metadata": {
        "id": "Vkjaa6lniZUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Run ALL Sentence-Level Feature Extraction\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "\n",
        "# Task 1 (Standard) lists\n",
        "hubert_features_list = []\n",
        "mfcc_features_list = []\n",
        "labels_list = []\n",
        "\n",
        "# Task 1 (Layer-wise) lists\n",
        "layer_features_list = [[] for _ in range(13)] # 13 layers (0 to 12)\n",
        "labels_list_layers = [] # Labels will be the same\n",
        "\n",
        "print(f\"Starting all feature extraction for {len(all_file_paths)} files...\")\n",
        "\n",
        "for path in tqdm(all_file_paths, desc=\"Extracting All Sentence Features\"):\n",
        "    try:\n",
        "        label_string = os.path.basename(os.path.dirname(path))\n",
        "        label_int = label_to_int[label_string]\n",
        "        waveform_raw, sr_raw = librosa.load(path, sr=None)\n",
        "\n",
        "        # --- 1. Extract MFCCs ---\n",
        "        mfccs = librosa.feature.mfcc(y=waveform_raw, sr=sr_raw, n_mfcc=20)\n",
        "        mfccs_mean = np.mean(mfccs, axis=1)\n",
        "        mfcc_features_list.append(mfccs_mean)\n",
        "\n",
        "        # --- 2. Resample for HuBERT ---\n",
        "        if sr_raw != target_sr:\n",
        "            waveform_16k = librosa.resample(waveform_raw, orig_sr=sr_raw, target_sr=target_sr)\n",
        "        else:\n",
        "            waveform_16k = waveform_raw\n",
        "\n",
        "        inputs = feature_extractor(waveform_16k, sampling_rate=target_sr, return_tensors='pt').to(device)\n",
        "\n",
        "        # --- 3. Extract Standard HuBERT (from base model) ---\n",
        "        with torch.no_grad():\n",
        "            outputs = hubert_model(inputs.input_values)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy()\n",
        "        hubert_features_list.append(embedding)\n",
        "        labels_list.append(label_int)\n",
        "\n",
        "        # --- 4. Extract Layer-wise HuBERT ---\n",
        "        with torch.no_grad():\n",
        "            outputs_layers = hubert_model_layers(inputs.input_values)\n",
        "\n",
        "        for i in range(13):\n",
        "            layer_embedding = outputs_layers.hidden_states[i].mean(dim=1).squeeze(0).cpu().numpy()\n",
        "            layer_features_list[i].append(layer_embedding)\n",
        "        labels_list_layers.append(label_int)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {path} due to error: {e}\")\n",
        "\n",
        "# --- Convert Task 1 (Standard) to NumPy ---\n",
        "X_hubert = np.array(hubert_features_list)\n",
        "X_mfcc = np.array(mfcc_features_list)\n",
        "y_labels = np.array(labels_list)\n",
        "print(f\"\\nStandard Feature extraction complete.\")\n",
        "print(f\"HuBERT shape: {X_hubert.shape}, MFCC shape: {X_mfcc.shape}\")\n",
        "\n",
        "# --- Convert Task 1 (Layer-wise) to NumPy ---\n",
        "X_hubert_layers = [np.array(features) for features in layer_features_list]\n",
        "y_labels_layers = np.array(labels_list_layers)\n",
        "print(f\"Layer-wise extraction complete. Created {len(X_hubert_layers)} feature matrices.\")"
      ],
      "metadata": {
        "id": "oxnqWfXNigDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Mount Drive & Save All Features\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "try:\n",
        "    print(\"Saving processed data to your Google Drive...\")\n",
        "    save_path = '/content/drive/MyDrive/Colab_Project_Data/'\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    np.save(os.path.join(save_path, 'X_hubert.npy'), X_hubert)\n",
        "    np.save(os.path.join(save_path, 'X_mfcc.npy'), X_mfcc)\n",
        "    np.save(os.path.join(save_path, 'y_labels.npy'), y_labels)\n",
        "    # Save the layer-wise data\n",
        "    np.save(os.path.join(save_path, 'X_hubert_layers.npy'), np.array(X_hubert_layers, dtype=object))\n",
        "\n",
        "    with open(os.path.join(save_path, 'label_to_int.json'), 'w') as f:\n",
        "        json.dump(label_to_int, f)\n",
        "\n",
        "    print(f\"--- SUCCESS --- All data saved to: {save_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while saving: {e}\")"
      ],
      "metadata": {
        "id": "dSzYMKGOijFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Train & Compare SVMs (Task 1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "print(\"--- Task 1: MFCC vs. HuBERT (Sentence-Level) ---\")\n",
        "X_mfcc_train, X_mfcc_test, y_train, y_test = train_test_split(\n",
        "    X_mfcc, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        ")\n",
        "X_hubert_train, X_hubert_test, _, _ = train_test_split(\n",
        "    X_hubert, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        ")\n",
        "\n",
        "mfcc_scaler = StandardScaler().fit(X_mfcc_train)\n",
        "hubert_scaler = StandardScaler().fit(X_hubert_train)\n",
        "\n",
        "print(\"Training MFCC Model...\")\n",
        "mfcc_svm = SVC(random_state=42).fit(mfcc_scaler.transform(X_mfcc_train), y_train)\n",
        "y_pred_mfcc = mfcc_svm.predict(mfcc_scaler.transform(X_mfcc_test))\n",
        "acc_mfcc = accuracy_score(y_test, y_pred_mfcc)\n",
        "\n",
        "print(\"Training HuBERT Model...\")\n",
        "hubert_svm = SVC(random_state=42).fit(hubert_scaler.transform(X_hubert_train), y_train)\n",
        "y_pred_hubert = hubert_svm.predict(hubert_scaler.transform(X_hubert_test))\n",
        "acc_hubert = accuracy_score(y_test, y_pred_hubert)\n",
        "\n",
        "print(\"\\n--- COMPARISON (Task 1) ---\")\n",
        "print(f\"MFCC Model Accuracy:   {acc_mfcc * 100:.2f}%\")\n",
        "print(f\"HuBERT Model Accuracy: {acc_hubert * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "5uv5WgnpilXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Train & Plot Layer-wise SVMs (Task 1.3)\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "layer_accuracies = []\n",
        "layer_indices = range(len(X_hubert_layers))\n",
        "\n",
        "print(\"Starting layer-wise model training...\")\n",
        "for i in layer_indices:\n",
        "    print(f\"Training on Layer {i}...\")\n",
        "    X_layer = X_hubert_layers[i]\n",
        "    y_labels = y_labels_layers\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_layer, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        "    )\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    svm = SVC(random_state=42).fit(scaler.transform(X_train), y_train)\n",
        "    y_pred = svm.predict(scaler.transform(X_test))\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    layer_accuracies.append(acc)\n",
        "print(\"Layer-wise training complete.\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(layer_indices, layer_accuracies, marker='o', linestyle='-')\n",
        "plt.title('HuBERT Layer-wise Analysis for Accent Identification')\n",
        "plt.xlabel('HuBERT Layer (0=Input, 1-12=Transformer)')\n",
        "plt.ylabel('Accent Classification Accuracy')\n",
        "plt.xticks(layer_indices)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "best_layer_index = np.argmax(layer_accuracies)\n",
        "best_accuracy = layer_accuracies[best_layer_index]\n",
        "print(f\"\\n--- Task 1 (Part 3) Result ---\")\n",
        "print(f\"Best performance found at Layer: {best_layer_index} with {best_accuracy * 100:.2f}% accuracy.\")"
      ],
      "metadata": {
        "id": "KHkBGMEPinEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}